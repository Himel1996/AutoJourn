{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"news_app_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump</td>\n",
       "      <td>FloppySlapper: If you voted for Trump, this is...</td>\n",
       "      <td>President Trump Decisions and Policies</td>\n",
       "      <td>This text appears to be a collection of posts ...</td>\n",
       "      <td>Title: Enthusiasm for Trump's Policies Continu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ukraine war</td>\n",
       "      <td>Trump says Ukraine 'should never have started ...</td>\n",
       "      <td>Ukraine War</td>\n",
       "      <td>This conversation appears to be discussing the...</td>\n",
       "      <td>Title: Deepening Concerns Over Trump's Alleged...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ukraine war</td>\n",
       "      <td>Trump says Ukraine 'should never have started ...</td>\n",
       "      <td>Political Analysis - Ukraine Conflict</td>\n",
       "      <td>It appears that this conversation revolves aro...</td>\n",
       "      <td>Title: Deepening Concerns Over Trump's Alleged...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM</td>\n",
       "      <td>AdventurousMistake72: What are people running ...</td>\n",
       "      <td>Applying to Law Schools (LLM)</td>\n",
       "      <td>Hello! It seems like you have a mix of persona...</td>\n",
       "      <td>Title: Stanford vs. Georgetown: The Tough Choi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diamonds</td>\n",
       "      <td>NoopKit: Why do people still buy diamonds? ver...</td>\n",
       "      <td>Diamond Market Scarcity and Pricing</td>\n",
       "      <td>The discussion revolves around the perceived s...</td>\n",
       "      <td>Exploring the Shift in the Diamond Market: Fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>Pyromania1983: [Serious] What specifically is ...</td>\n",
       "      <td>Bob Dylan Personality</td>\n",
       "      <td>From the conversation, it appears that Bob Dyl...</td>\n",
       "      <td>Title: Unpacking the Enigma That Is Bob Dylan:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Timothee Chalamet</td>\n",
       "      <td>vitkurd: Timothee Chalamet with Kylie Jenner a...</td>\n",
       "      <td>Movie Review: Timothee Chalamet Career Analysis</td>\n",
       "      <td>The conversation appears to be about a video m...</td>\n",
       "      <td>Headline: Timothée Chalamet's Rising Star: A R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Van Gogh</td>\n",
       "      <td>Snorlax_Cuddles: When did people actually real...</td>\n",
       "      <td>Vincent van Gogh</td>\n",
       "      <td>It looks like you've shared a collection of co...</td>\n",
       "      <td>Title: A Timeless Gaze: The Enduring Impact of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Economy</td>\n",
       "      <td>Rebelliousdefender: I hate the lies about the ...</td>\n",
       "      <td>Economy and Business</td>\n",
       "      <td>The text appears to be a collection of comment...</td>\n",
       "      <td>Title: America's Economic and Political Landsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Corona virus</td>\n",
       "      <td>mostrandomguy: Medical professionals around th...</td>\n",
       "      <td>COVID-19 Testing Issues in the US</td>\n",
       "      <td>It appears you've shared a discussion thread a...</td>\n",
       "      <td>Title: U.S. Struggles to Overcome Testing Shor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Corona virus</td>\n",
       "      <td>mostrandomguy: Medical professionals around th...</td>\n",
       "      <td>COVID-19 Recovery Stories</td>\n",
       "      <td>In this conversation, people are discussing th...</td>\n",
       "      <td>A Beacon of Hope: Elderly Woman Defies Odds, B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                query                                               text  \\\n",
       "0               Trump  FloppySlapper: If you voted for Trump, this is...   \n",
       "1         Ukraine war  Trump says Ukraine 'should never have started ...   \n",
       "2         Ukraine war  Trump says Ukraine 'should never have started ...   \n",
       "3                 LLM  AdventurousMistake72: What are people running ...   \n",
       "4            Diamonds  NoopKit: Why do people still buy diamonds? ver...   \n",
       "5           Bob Dylan  Pyromania1983: [Serious] What specifically is ...   \n",
       "6   Timothee Chalamet  vitkurd: Timothee Chalamet with Kylie Jenner a...   \n",
       "7            Van Gogh  Snorlax_Cuddles: When did people actually real...   \n",
       "8             Economy  Rebelliousdefender: I hate the lies about the ...   \n",
       "9        Corona virus  mostrandomguy: Medical professionals around th...   \n",
       "10       Corona virus  mostrandomguy: Medical professionals around th...   \n",
       "\n",
       "                                              topic  \\\n",
       "0            President Trump Decisions and Policies   \n",
       "1                                       Ukraine War   \n",
       "2             Political Analysis - Ukraine Conflict   \n",
       "3                     Applying to Law Schools (LLM)   \n",
       "4               Diamond Market Scarcity and Pricing   \n",
       "5                             Bob Dylan Personality   \n",
       "6   Movie Review: Timothee Chalamet Career Analysis   \n",
       "7                                  Vincent van Gogh   \n",
       "8                              Economy and Business   \n",
       "9                 COVID-19 Testing Issues in the US   \n",
       "10                        COVID-19 Recovery Stories   \n",
       "\n",
       "                                              summary  \\\n",
       "0   This text appears to be a collection of posts ...   \n",
       "1   This conversation appears to be discussing the...   \n",
       "2   It appears that this conversation revolves aro...   \n",
       "3   Hello! It seems like you have a mix of persona...   \n",
       "4   The discussion revolves around the perceived s...   \n",
       "5   From the conversation, it appears that Bob Dyl...   \n",
       "6   The conversation appears to be about a video m...   \n",
       "7   It looks like you've shared a collection of co...   \n",
       "8   The text appears to be a collection of comment...   \n",
       "9   It appears you've shared a discussion thread a...   \n",
       "10  In this conversation, people are discussing th...   \n",
       "\n",
       "                                                 news  \n",
       "0   Title: Enthusiasm for Trump's Policies Continu...  \n",
       "1   Title: Deepening Concerns Over Trump's Alleged...  \n",
       "2   Title: Deepening Concerns Over Trump's Alleged...  \n",
       "3   Title: Stanford vs. Georgetown: The Tough Choi...  \n",
       "4   Exploring the Shift in the Diamond Market: Fro...  \n",
       "5   Title: Unpacking the Enigma That Is Bob Dylan:...  \n",
       "6   Headline: Timothée Chalamet's Rising Star: A R...  \n",
       "7   Title: A Timeless Gaze: The Enduring Impact of...  \n",
       "8   Title: America's Economic and Political Landsc...  \n",
       "9   Title: U.S. Struggles to Overcome Testing Shor...  \n",
       "10  A Beacon of Hope: Elderly Woman Defies Odds, B...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text=df[\"news\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: A Timeless Gaze: The Enduring Impact of Vincent van Gogh\\'s Artistry\\n\\n\\nIn a world brimming with visual stimuli, where the fleeting and ephemeral often dominate our collective consciousness, there remains one artist whose work continues to resonate profoundly with millions. Vincent van Gogh, the Dutch post-Impressionist master, has captivated audiences for over a century, his paintings serving as enduring testaments to the power of emotion, color, and human connection.\\n\\n\\nVan Gogh\\'s oeuvre is characterized by vibrant hues that dance upon the canvas, textures that beg to be touched, and an emotional intensity that transcends time and space. These elements, combined with his distinctive brushstrokes and evocative subject matter, have ensured that van Gogh\\'s work remains as relevant today as it was over a century ago.\\n\\n\\nOne of the most iconic pieces in van Gogh\\'s repertoire is \"Starry Night,\" a swirling cosmic tableau that has become synonymous with his name. This painting, created during a turbulent period in van Gogh\\'s life, serves as a poignant reminder of the artist\\'s struggle to find meaning and solace amidst the chaos of existence. The swirling stars and tumultuous skies are not mere depictions of the night sky but rather, an expression of van Gogh\\'s inner turmoil and his yearning for spiritual connection.\\n\\n\\nAnother beloved work is van Gogh\\'s wheat field series, particularly the version housed at the National Gallery in London. The undulating waves of wheat stretch out across the canvas, their golden hues evoking a sense of tranquility and serenity. These paintings serve as poignant reminders of van Gogh\\'s deep connection to nature and his ability to capture its essence with unparalleled skill.\\n\\n\\nVan Gogh\\'s work often invites comparison with other masters of the Impressionist and Post-Impressionist movements, most notably Monet and Matisse. While each artist possessed a unique vision, van Gogh stood out for his emotional resonance and his ability to imbue even the simplest of subjects with depth and meaning.\\n\\n\\nThe appeal of van Gogh\\'s work is not limited to admiration from afar. Many have traveled great distances to experience the visceral impact of his paintings in person, venturing across countries and continents to bear witness to these hallowed masterpieces. The unique qualities that can only be appreciated up close—the intricate brushstrokes, the rich textures, the subtle nuances in color—make these journeys worthwhile for those seeking a deeper connection with van Gogh\\'s work and, by extension, with themselves.\\n\\n\\nVan Gogh\\'s art serves as a testament to the power of human emotion and the transformative potential of creativity. His paintings have not only captured the hearts and minds of countless admirers but have also inspired generations of artists who have sought to emulate his passion, his vision, and his unwavering commitment to his craft.\\n\\n\\nAs we stand before these masterpieces, whether in a museum or gallery, we are reminded of the enduring power of art and its ability to transcend time, space, and cultural boundaries. Vincent van Gogh\\'s work continues to speak to us, urging us to look within ourselves and to find our own connections with the world around us. And as long as his paintings hang upon the walls of galleries across the globe, they will continue to inspire and captivate, reminding us of the limitless potential of human creativity and the enduring power of emotion.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_document=df[\"text\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Snorlax_Cuddles: When did people actually realise the value of Van Gogh? AutoModerator: Welcome to /r/AskHistorians. **Please [Read Our Rules](https://www.reddit.com/r/AskHistorians/wiki/rules) before you comment in this community**. Understand that [rule breaking comments get removed](https://www.reddit.com/r/AskHistorians/comments/h8aefx/rules_roundtable_xviii_removed_curation_and_why/). #Please consider **[Clicking Here for RemindMeBot](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps://www.reddit.com/r/AskHistorians/comments/1b7ahrs/when_did_people_actually_realise_the_value_of_van/%5D%0A%0ARemindMe!%202%20days)** as it takes time for an answer to be written. Additionally, for weekly content summaries, **[Click Here to Subscribe to our Weekly Roundup](https://www.reddit.com/message/compose/?to=AHMessengerBot&subject=Subscribe&message=!subscribe)**. We thank you for your interest in this *question*, and your patience in waiting for an in-depth and comprehensive answer to show up. In addition to RemindMeBot, consider [using our Browser Extension](https://www.reddit.com/r/AskHistorians/comments/d6dzi7/tired_of_clicking_to_find_only_removed_comments/), or getting the [Weekly Roundup](https://www.reddit.com/message/compose?to=subredditsummarybot&subject=askhistorians+weekly&message=x). In the meantime our [Twitter](https://twitter.com/askhistorians), [Facebook](https://www.facebook.com/askhistorians/), and [Sunday Digest](https://www.reddit.com/r/AskHistorians/search?q=title%3A%22Sunday+Digest%22&restrict_sr=on&sort=new&t=all) feature excellent content that has already been written! *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskHistorians) if you have any questions or concerns.* Sneakys2: When Van Gogh passed, his brother Theo was the executor of his estate and inherited all of his brother’s work. At the time of Van Gogh’s death, he was already attracting attention from critics. His work was shown in multiple salons between 1888-1890 (note that he died in 1890).  It should be noted that Van Gogh’s obscurity is relative. While the average person would not have known much about him, the average person would have also known very little about Monet or Matisse or many of the other modernist painters. During his life, Van Gogh corresponded with other artists, including Gauguin, and his work was held in high regard by many of them. By the time WWI began, he had a reputation as a major modernist artist. At the time he took his life, he was already beginning to see some commercial success as a painter.  His brother died in 1891 and it’s actually Theo’s widow who sold the bulk of Vincent’s works. She sold to a number of collectors including Paul Cassirer. Van Gogh was particularly popular in German collections prior to WWI. This was assisted by the early interest of the art historian Julius Meier-Graefe, who wrote some of the earliest art historical literature on Van Gogh. While Van Gogh’s slowly increased in value throughout the 20th century, there was a significant boom in the 1980s when the Japanese market became enraptured by him (and other impressionist and post-impressionist painters). That’s when you start to see the absurd prices that more or less have continued until today.  >Point A of course is that a Van Gogh is worth a bus pass and half a ham sandwich. There’s a perception about Van Gogh and other artists that they were poor and starving. This is a major misconception. It takes a certain amount of money to be an artist. You need to be able to afford materials and space and time in order to create art. The truly destitute do not become artists. Many of the “starving artists” that people think of are able to be artists because they have family support. In the case of Van Gogh, his brother Theo provided his financial support. Theo paid for Vincent’s stays in asylums and made sure his brother had food to eat and lodging. While Van Gogh never got rich off his work, he definitely was able to be supported which allowed for him to focus on his artwork.  KaiLung: I asked a [similar question](https://www.reddit.com/r/AskHistorians/comments/m3khby/why_did_van_gogh_have_so_little_success_selling/) a while ago and got some feedback from u/2deux2. jaidit: One thing that tends to get neglected is we really need to look at Van Gogh through a modern lens (so often in history it’s “take off your modern glasses,” but here it’s “leave them on please.” Let’s say you want to buy a Damien Hirst. You meet Hirst at a party and…you have to talk to his gallery. Van Gogh had something very modern: gallery representation. The Van Gogh brothers worked for their uncle and they hit upon the idea of their own gallery. Theo would manage the business. The reason Vincent only sold five paintings (on his own) was that Theo owned the rest of them. He trying to build Vincent’s reputation to get the best price. And so there were letters telling Vincent that somber palettes were hard to sell. People want something bright in their drawing rooms, Vincent. On occasion, Theo would trade Vincent’s work for that of other artists (if you’re not selling your Van Goghs, you need something). These few trades and sales account for the bulk of Van Goghs outside the Van Gogh museum. After the deaths of Vincent (suicide) and Theo (syphilis), Theo’s widow Jo inherited everything. She kept almost all of it. A few gallery shows and a little myth-making accomplished what her husband could not. Lanpenn_: Why did the Japanese market become interested on Van Gogh\\'s works? KaiLung: Van Gogh was deeply influenced by Japanese Ukiyoe art and owned a large collection of prints, and even painted copies of some of them. Van Gogh wrote to Theo, \"\"All my work is based to some extent on Japanese art.\" There\\'s an article on the Van Gogh Museum page about Van Gogh\\'s influence - [https://www.vangoghmuseum.nl/en/stories/inspiration-from-japan#0](https://www.vangoghmuseum.nl/en/stories/inspiration-from-japan#0)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Creative Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we train a distil Bert classifier model on a classification downstream task for classifying between creative/subjective and factual texts. We use the trained model in the hallucination detection pipeline to only select factual texts and detect hallucinations in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samples = [\n",
    "    {\"text\": \"In a world brimming with possibilities, I feel hope at every turn.\", \"label\": 1},\n",
    "    {\"text\": \"This painting evokes a deep emotional resonance in my heart.\", \"label\": 1},\n",
    "    {\"text\": \"The colors in this picture reflect a boundless imagination.\", \"label\": 1},\n",
    "    {\"text\": \"Paris is the capital city of France.\", \"label\": 0},\n",
    "    {\"text\": \"Vincent van Gogh was born in 1853.\", \"label\": 0},\n",
    "    {\"text\": \"The population of New York City is approximately 8.4 million.\", \"label\": 0},\n",
    "    {\"text\": \"That sunset is the most beautiful thing I’ve ever seen!\", \"label\": 1},\n",
    "    {\"text\": \"The Great Wall of China extends more than 13,000 miles.\", \"label\": 0},\n",
    "    {\"text\": \"I find these brushstrokes utterly mesmerizing and enchanting.\", \"label\": 1},\n",
    "    {\"text\": \"The Louvre Museum is located in Paris, France.\", \"label\": 0},\n",
    "    {\"text\": \"Her laughter was a melody that danced through the autumn air.\", \"label\": 1},\n",
    "    {\"text\": \"The weight of the world seemed to lift as he stepped into the sunlight.\", \"label\": 1},\n",
    "    {\"text\": \"Each brushstroke tells a story of passion and turmoil.\", \"label\": 1},\n",
    "    {\"text\": \"In the silence of the night, the stars whispered secrets to my soul.\", \"label\": 1},\n",
    "    {\"text\": \"This poem wraps around my heart like a gentle embrace.\", \"label\": 1},\n",
    "    {\"text\": \"The aroma of coffee in the morning is pure bliss.\", \"label\": 1},\n",
    "    {\"text\": \"His words cut deeper than any knife, leaving scars unseen.\", \"label\": 1},\n",
    "    {\"text\": \"The chaos of the city streets mirrors the turmoil within me.\", \"label\": 1},\n",
    "    {\"text\": \"A single rose can symbolize love, loss, and hope all at once.\", \"label\": 1},\n",
    "    {\"text\": \"The waves crashed against the shore in a furious, untamed dance.\", \"label\": 1},\n",
    "    {\"text\": \"Memories of childhood summers are painted in hues of gold and green.\", \"label\": 1},\n",
    "    {\"text\": \"Her eyes held galaxies, swirling with unspoken stories.\", \"label\": 1},\n",
    "    {\"text\": \"The old book smelled like secrets waiting to be discovered.\", \"label\": 1},\n",
    "    {\"text\": \"Time stood still as their eyes met across the crowded room.\", \"label\": 1},\n",
    "    {\"text\": \"The melody carried a bittersweet ache that resonated in my chest.\", \"label\": 1},\n",
    "    {\"text\": \"The Amazon River is the second longest river in the world.\", \"label\": 0},\n",
    "    {\"text\": \"Albert Einstein developed the theory of relativity.\", \"label\": 0},\n",
    "    {\"text\": \"Water boils at 100 degrees Celsius at sea level.\", \"label\": 0},\n",
    "    {\"text\": \"Mount Everest's summit is 8,848 meters above sea level.\", \"label\": 0},\n",
    "    {\"text\": \"The United Nations was established in 1945.\", \"label\": 0},\n",
    "    {\"text\": \"The chemical symbol for gold is Au.\", \"label\": 0},\n",
    "    {\"text\": \"Tokyo is the capital city of Japan.\", \"label\": 0},\n",
    "    {\"text\": \"The human skeleton consists of 206 bones.\", \"label\": 0},\n",
    "    {\"text\": \"The Eiffel Tower was completed in 1889.\", \"label\": 0},\n",
    "    {\"text\": \"The Pacific Ocean is the largest ocean on Earth.\", \"label\": 0},\n",
    "    {\"text\": \"The Statue of Liberty was a gift from France to the United States.\", \"label\": 0},\n",
    "    {\"text\": \"The square root of 144 is 12.\", \"label\": 0},\n",
    "    {\"text\": \"The currency of Japan is the yen.\", \"label\": 0},\n",
    "    {\"text\": \"Mars is often called the 'Red Planet' due to its reddish appearance.\", \"label\": 0},\n",
    "    {\"text\": \"The human body has four chambers in the heart.\", \"label\": 0},\n",
    "    {\"text\": \"The human brain weighs approximately 1.4 kilograms on average.\", \"label\": 0},\n",
    "    {\"text\": \"Her tears were raindrops mourning the end of summer.\", \"label\": 1},\n",
    "    {\"text\": \"The Sahara Desert is the largest hot desert in the world.\", \"label\": 0},\n",
    "    {\"text\": \"The moonlight draped the forest in a silver veil of secrets.\", \"label\": 1},\n",
    "    {\"text\": \"Shakespeare wrote 37 plays, including 'Hamlet' and 'Macbeth'.\", \"label\": 0},\n",
    "    {\"text\": \"The wind carried whispers of forgotten promises.\", \"label\": 1},\n",
    "    {\"text\": \"The atomic number of carbon is 6.\", \"label\": 0},\n",
    "    {\"text\": \"His anger was a wildfire, consuming everything in its path.\", \"label\": 1},\n",
    "    {\"text\": \"The capital of Australia is Canberra.\", \"label\": 0},\n",
    "    {\"text\": \"The old clock tower stood as a sentinel of time's relentless march.\", \"label\": 1},\n",
    "    {\"text\": \"The first moon landing occurred in 1969.\", \"label\": 0},\n",
    "    {\"text\": \"The aroma of old books is a symphony of dust and nostalgia.\", \"label\": 1},\n",
    "    {\"text\": \"The speed of light in a vacuum is 299,792,458 meters per second.\", \"label\": 0},\n",
    "    {\"text\": \"Her voice was a lighthouse guiding me through the fog of doubt.\", \"label\": 1},\n",
    "    {\"text\": \"The currency of Germany before the euro was the Deutsche Mark.\", \"label\": 0},\n",
    "    {\"text\": \"The shadows in the room seemed to breathe with hidden stories.\", \"label\": 1},\n",
    "    {\"text\": \"The human body has 24 ribs.\", \"label\": 0},\n",
    "    {\"text\": \"The autumn leaves blazed like embers in the twilight.\", \"label\": 1},\n",
    "    {\"text\": \"The Nile River flows through 11 African countries.\", \"label\": 0},\n",
    "    {\"text\": \"The melody of the piano keys stirred a tempest in my soul.\", \"label\": 1},\n",
    "    {\"text\": \"The Great Pyramid of Giza is the oldest of the Seven Wonders.\", \"label\": 0},\n",
    "    {\"text\": \"The silence between us screamed louder than any words.\", \"label\": 1},\n",
    "    {\"text\": \"The chemical formula for table salt is NaCl.\", \"label\": 0},\n",
    "    {\"text\": \"The stars blinked like diamonds scattered on velvet.\", \"label\": 1},\n",
    "    {\"text\": \"The Magna Carta was signed in 1215.\", \"label\": 0},\n",
    "    {\"text\": \"The fog clung to the hills like a lover unwilling to part.\", \"label\": 1},\n",
    "    {\"text\": \"The diameter of Earth is about 12,742 kilometers.\", \"label\": 0},\n",
    "    {\"text\": \"Her courage was a flame that darkness could not extinguish.\", \"label\": 1},\n",
    "    {\"text\": \"The first photograph was taken in 1826 by Joseph Nicéphore Niépce.\", \"label\": 0},\n",
    "    {\"text\": \"The old oak tree stood as a silent guardian of the village's forgotten tales.\", \"label\": 1},\n",
    "    {\"text\": \"The planet Mercury has no moons.\", \"label\": 0},\n",
    "    {\"text\": \"The taste of childhood lingered in the sweetness of the homemade pie.\", \"label\": 1},\n",
    "    {\"text\": \"The capital of Canada is Ottawa.\", \"label\": 0},\n",
    "    {\"text\": \"The thunder roared like a wounded beast in the distance.\", \"label\": 1},\n",
    "    {\"text\": \"The human eye can distinguish about 10 million colors.\", \"label\": 0},\n",
    "    {\"text\": \"The city skyline glittered like a fractured constellation.\", \"label\": 1},\n",
    "    {\"text\": \"The Wright brothers' first flight lasted 12 seconds.\", \"label\": 0},\n",
    "    {\"text\": \"Her smile was a sunrise after a stormy night.\", \"label\": 1},\n",
    "    {\"text\": \"The freezing point of water is 0 degrees Celsius.\", \"label\": 0},\n",
    "    {\"text\": \"The fireflies waltzed in the twilight, painting the air with gold.\", \"label\": 1},\n",
    "    {\"text\": \"The Roman Colosseum could hold up to 80,000 spectators.\", \"label\": 0},\n",
    "    {\"text\": \"The weight of unspoken words hung heavy in the room.\", \"label\": 1},\n",
    "    {\"text\": \"Honey never spoils; archaeologists have found edible honey in ancient tombs.\", \"label\": 0},\n",
    "    {\"text\": \"The river’s current sang a lullaby to the weary traveler.\", \"label\": 1},\n",
    "    {\"text\": \"The first electronic computer was invented in the 1940s.\", \"label\": 0},\n",
    "    {\"text\": \"The abandoned house creaked with the ghosts of laughter long gone.\", \"label\": 1},\n",
    "    {\"text\": \"The Hawaiian alphabet has only 13 letters.\", \"label\": 0},\n",
    "    {\"text\": \"The storm raged like a broken heart, fierce and unyielding.\", \"label\": 1},\n",
    "    {\"text\": \"Venus is the hottest planet in our solar system.\", \"label\": 0},\n",
    "    {\"text\": \"Memories of her lingered like the scent of rain on dry earth.\", \"label\": 1},\n",
    "    {\"text\": \"The first telephone call was made by Alexander Graham Bell in 1876.\", \"label\": 0},\n",
    "    {\"text\": \"The candle’s flame flickered like a tiny dancer in the dark.\", \"label\": 1},\n",
    "    {\"text\": \"The human liver can regenerate itself even after 75% removal.\", \"label\": 0},\n",
    "    {\"text\": \"The mountain peaks pierced the clouds like ancient sentinels.\", \"label\": 1},\n",
    "    {\"text\": \"The currency of Sweden is the Swedish krona.\", \"label\": 0},\n",
    "    {\"text\": \"The old bridge sighed under the weight of a thousand footsteps.\", \"label\": 1},\n",
    "    {\"text\": \"The shortest war in history lasted 38 minutes between Zanzibar and England.\", \"label\": 0},\n",
    "    {\"text\": \"The dawn bled crimson across the horizon, raw and unapologetic.\", \"label\": 1},\n",
    "    {\"text\": \"The human body produces about 1 liter of saliva daily.\", \"label\": 0},\n",
    "    {\"text\": \"The rusted gate groaned like a forgotten memory.\", \"label\": 1},\n",
    "    {\"text\": \"Every small act of kindness ripples outward, shaping worlds we may never see.\", \"label\": 1},\n",
    "    {\"text\": \"A society that forgets its poets risks losing its soul to the cold machinery of progress.\", \"label\": 1},\n",
    "    {\"text\": \"True leadership isn’t measured in policies enacted, but in hearts ignited and minds awakened.\", \"label\": 1},\n",
    "    {\"text\": \"The brushstrokes of rebellion in his art scream louder than any protest march ever could.\", \"label\": 1},\n",
    "    {\"text\": \"We stand at a crossroads where apathy could drown the last whispers of hope.\", \"label\": 1},\n",
    "    {\"text\": \"Her perseverance became a beacon for those lost in the fog of self-doubt.\", \"label\": 1},\n",
    "    {\"text\": \"The true cost of silence is measured in generations yet unborn.\", \"label\": 1},\n",
    "    {\"text\": \"In the tension between security and freedom, the soul of democracy trembles.\", \"label\": 1},\n",
    "    {\"text\": \"The echoes of that decision would haunt policy discussions for decades to come.\", \"label\": 1},\n",
    "    {\"text\": \"A nation's character is revealed not in victory parades, but in its treatment of the marginalized.\", \"label\": 1},\n",
    "    {\"text\": \"The rhythm of protest songs still pulses through the veins of social movements.\", \"label\": 1},\n",
    "    {\"text\": \"Weaving through bureaucratic red tape often feels like battling mythical beasts from ancient lore.\", \"label\": 1},\n",
    "    {\"text\": \"The weight of unspoken truths can bend even the strongest political will.\", \"label\": 1},\n",
    "    {\"text\": \"His lyrics became the anthem for a generation staring down the barrel of climate anxiety.\", \"label\": 1},\n",
    "    {\"text\": \"The chasm between political rhetoric and lived reality grows wider with each empty promise.\", \"label\": 1},\n",
    "    {\"text\": \"In the quiet spaces between policy drafts, the human cost often gets erased.\", \"label\": 1},\n",
    "    {\"text\": \"The fire of grassroots movements often starts with a single spark of discontent.\", \"label\": 1},\n",
    "    {\"text\": \"Political legacies are written not in legislation, but in the collective memory of those impacted.\", \"label\": 1},\n",
    "    {\"text\": \"The city's heartbeat pulses with untold stories of resilience beneath its crumbling infrastructure.\", \"label\": 1},\n",
    "    {\"text\": \"Every closed factory door echoes with the ghosts of abandoned dreams.\", \"label\": 1},\n",
    "    {\"text\": \"The true test of democracy lies in how it weathers the storms of misinformation.\", \"label\": 1},\n",
    "    {\"text\": \"Her paintings scream the colors of resistance that her voice could never articulate.\", \"label\": 1},\n",
    "    {\"text\": \"In the shadow of geopolitical chess games, ordinary lives become pawns in someone else's strategy.\", \"label\": 1},\n",
    "    {\"text\": \"The melody of progress often falters when met with the discord of entrenched power.\", \"label\": 1},\n",
    "    {\"text\": \"A single policy shift can unravel the fragile tapestry of community trust woven over decades.\", \"label\": 1}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = Dataset.from_dict({\n",
    "    \"text\": [item[\"text\"] for item in dataset_samples],\n",
    "    \"label\": [item[\"label\"] for item in dataset_samples]\n",
    "})\n",
    "\n",
    "# Split into train/test (80/20 here, but small for demo)\n",
    "# Split using Hugging Face's native method\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_data = split_dataset[\"train\"]\n",
    "test_data = split_dataset[\"test\"]\n",
    "\n",
    "# Convert train/test splits into a DatasetDict\n",
    "train_dataset = Dataset.from_dict(train_data.to_dict())\n",
    "test_dataset = Dataset.from_dict(test_data.to_dict())\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb5d9c5e7a7452f814ef7445536d842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/248 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b3cd04bde148f693201840be7e806f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/1.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb406bfe6eb04fdfacbe3c3c525707a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/364k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dc2567669a4132aa5e0e7f86bf0e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312ca07281e2462d83d730a4a3b780b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"SetFit/subj\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 6400\n",
      "Test samples: 1600\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset  = dataset[\"test\"]\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8755bfe8bf1e4a8ab2740f0fabe10c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adacc57da4f47eeb98dd338e9e5351a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himel/miniconda3/envs/graco/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./subjective_model_large\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,     # Increase if you want more training\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3403295/3800285915.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on a larger dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 03:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.237178</td>\n",
       "      <td>0.960625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.287566</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.249028</td>\n",
       "      <td>0.961250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.240699</td>\n",
       "      <td>0.959375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.368206</td>\n",
       "      <td>0.951875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358800</td>\n",
       "      <td>0.958750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373867</td>\n",
       "      <td>0.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.961250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.353510</td>\n",
       "      <td>0.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354670</td>\n",
       "      <td>0.962500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting training on a larger dataset...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/100 00:00 < 00:01, 62.33 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation results: {'eval_loss': 0.3535095155239105, 'eval_accuracy': 0.9625, 'eval_runtime': 1.5301, 'eval_samples_per_second': 1045.685, 'eval_steps_per_second': 65.355, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Test set evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjective/Creative\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def classify_subjectivity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"This painting evokes a deep emotional resonance in my heart.\"\n",
    "label = classify_subjectivity(sample_text)\n",
    "if label == 1:\n",
    "    print(\"Subjective/Creative\")\n",
    "else:\n",
    "    print(\"Objective/Factual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = \"./subjective_model_large/checkpoint-4000\"  # or wherever you saved your old model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = Dataset.from_dict({\n",
    "    \"text\": [item[\"text\"] for item in dataset_samples],\n",
    "    \"label\": [item[\"label\"] for item in dataset_samples]\n",
    "})\n",
    "\n",
    "# Split into train/test (80/20 here, but small for demo)\n",
    "# Split using Hugging Face's native method\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_data = split_dataset[\"train\"]\n",
    "test_data = split_dataset[\"test\"]\n",
    "\n",
    "# Convert train/test splits into a DatasetDict\n",
    "train_dataset = Dataset.from_dict(train_data.to_dict())\n",
    "test_dataset = Dataset.from_dict(test_data.to_dict())\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaa43f7165e488bb760c28d3d358391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62816706723f437b8286d2492313b9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himel/miniconda3/envs/graco/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./subjective_model_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,                # Adjust as needed\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3403295/4188618530.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting further fine-tuning on our full_dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.643979</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.053859</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.008330</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.010031</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.010831</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.011199</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting further fine-tuning on our full_dataset...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation results: {'eval_loss': 0.0021678770426660776, 'eval_accuracy': 1.0, 'eval_runtime': 0.0273, 'eval_samples_per_second': 915.116, 'eval_steps_per_second': 73.209, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Test set evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./subjective_model_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = \"./subjective_model_finetuned/checkpoint-70\"  # or wherever you saved your old model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Classifier Model in Fact Checking pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_subjectivity(text, model, tokenizer, device=device):\n",
    "    \"\"\"\n",
    "    Classify a piece of text as 0 => objective/factual, 1 => subjective/creative.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjective/Creative\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentence = \"Kolkata is the most beautiful city in my heart.\"\n",
    "label = classify_subjectivity(sentence, model, tokenizer, device=\"cuda:1\")\n",
    "if label == 1:\n",
    "    print(\"Subjective/Creative\")\n",
    "else:\n",
    "    print(\"Objective/Factual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import wikipedia\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# We'll reuse the Bart Large MNLI model for factual entailment\n",
    "entailment_model_name = \"facebook/bart-large-mnli\"\n",
    "entailment_tokenizer = AutoTokenizer.from_pretrained(entailment_model_name)\n",
    "entailment_model = AutoModelForSequenceClassification.from_pretrained(entailment_model_name).to(device)\n",
    "\n",
    "def extract_claims_spacy(text):\n",
    "    \"\"\"Split text into sentences using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "def compute_entailment_probability(premise, hypothesis):\n",
    "    \"\"\"Return the 'entailment' probability using Bart-Large-MNLI.\"\"\"\n",
    "    inputs = entailment_tokenizer.encode_plus(\n",
    "        premise, hypothesis, return_tensors='pt', \n",
    "        truncation=True, max_length=1024\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = entailment_model(**inputs).logits\n",
    "\n",
    "    # MNLI order: [contradiction, neutral, entailment]\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    entailment_prob = probs[2]\n",
    "    return float(entailment_prob)\n",
    "\n",
    "def simple_keyword_retrieval(claim, document, window=100):\n",
    "    \"\"\"Naive snippet retrieval from the source doc.\"\"\"\n",
    "    keywords = claim.split()[:3]\n",
    "    snippet = document\n",
    "    doc_lower = document.lower()\n",
    "    for kw in keywords:\n",
    "        idx = doc_lower.find(kw.lower())\n",
    "        if idx != -1:\n",
    "            start = max(0, idx - window)\n",
    "            end = min(len(document), idx + window)\n",
    "            return snippet[start:end]\n",
    "    return document  # fallback\n",
    "\n",
    "def retrieve_wikipedia_snippets(entity_or_query, max_snippets=3):\n",
    "    \"\"\"Retrieve paragraphs from Wikipedia for the top search result.\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        search_results = wikipedia.search(entity_or_query)\n",
    "        if not search_results:\n",
    "            return results\n",
    "        \n",
    "        page_title = search_results[0]\n",
    "        page = wikipedia.page(page_title)\n",
    "        paragraphs = page.content.split(\"\\n\\n\")\n",
    "        \n",
    "        for para in paragraphs[:max_snippets]:\n",
    "            results.append(para)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_claim_with_source(claim, source_document, threshold=0.7):\n",
    "    \"\"\"\n",
    "    1. Retrieve snippet from the user-provided source document.\n",
    "    2. Compute entailment probability.\n",
    "    3. Mark claim as hallucination if below threshold.\n",
    "    \"\"\"\n",
    "    snippet = simple_keyword_retrieval(claim, source_document)\n",
    "    entail_prob = compute_entailment_probability(snippet, claim)\n",
    "    is_hallucination = entail_prob < threshold\n",
    "    \n",
    "    return {\n",
    "        \"claim\": claim,\n",
    "        \"source_snippet\": snippet,\n",
    "        \"source_entailment\": entail_prob,\n",
    "        \"source_is_hallucination\": is_hallucination\n",
    "    }\n",
    "\n",
    "def check_claim_with_wikipedia(claim, threshold=0.7):\n",
    "    \"\"\"\n",
    "    If source doc can't confirm claim, check Wikipedia.\n",
    "    If any retrieved snippet's entailment >= threshold => not hallucination.\n",
    "    \"\"\"\n",
    "    doc_ents = nlp(claim)\n",
    "    entities = [ent.text for ent in doc_ents.ents]\n",
    "    if not entities:\n",
    "        # fallback: just treat entire claim as query\n",
    "        entities = [claim[:60]]\n",
    "    \n",
    "    best_entailment = 0.0\n",
    "    best_snippet = None\n",
    "    \n",
    "    for entity in entities:\n",
    "        paragraphs = retrieve_wikipedia_snippets(entity)\n",
    "        for para in paragraphs:\n",
    "            prob = compute_entailment_probability(para, claim)\n",
    "            if prob > best_entailment:\n",
    "                best_entailment = prob\n",
    "                best_snippet = para\n",
    "    \n",
    "    wiki_is_hallucination = (best_entailment < threshold)\n",
    "    return {\n",
    "        \"claim\": claim,\n",
    "        \"wikipedia_snippet\": best_snippet,\n",
    "        \"wikipedia_entailment\": best_entailment,\n",
    "        \"wikipedia_is_hallucination\": wiki_is_hallucination\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, the fact checking has been done robustly, except that we have two false positives which we further refine by google searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "def retrieve_google_snippets(query, api_key, cx, max_results=3):\n",
    "    \"\"\"\n",
    "    Query Google Custom Search JSON API for `query`.\n",
    "    Returns a list of short text snippets from the top results.\n",
    "    max_results: how many results to return at most.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"cx\": cx,\n",
    "        \"q\": query,\n",
    "        \"num\": max_results\n",
    "    }\n",
    "    \n",
    "    snippets = []\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        items = data.get(\"items\", [])\n",
    "        for item in items:\n",
    "            snippet = item.get(\"snippet\", \"\")\n",
    "            if snippet:\n",
    "                snippets.append(snippet)\n",
    "    except Exception as e:\n",
    "        print(f\"[Google Search Error]: {e}\")\n",
    "    \n",
    "    return snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_claim_with_google(claim, api_key, cx, threshold=0.7, max_results=3):\n",
    "    \"\"\"\n",
    "    Final fallback: \n",
    "    1) Query Google with the entire claim (or entity-based approach).\n",
    "    2) Retrieve short snippets from top search results.\n",
    "    3) Compute the best entailment score across those snippets.\n",
    "    4) If best >= threshold => not hallucination\n",
    "    \"\"\"\n",
    "    # For consistency, let's also attempt an entity-based approach as we did with Wikipedia\n",
    "    doc_ents = nlp(claim)\n",
    "    entities = [ent.text for ent in doc_ents.ents]\n",
    "    if not entities:\n",
    "        # fallback: just treat entire claim as the query\n",
    "        entities = [claim[:60]]\n",
    "    \n",
    "    best_entailment = 0.0\n",
    "    best_snippet = None\n",
    "    \n",
    "    for entity in entities:\n",
    "        # Retrieve Google search snippets\n",
    "        google_snippets = retrieve_google_snippets(entity, api_key, cx, max_results=max_results)\n",
    "        for snippet in google_snippets:\n",
    "            prob = compute_entailment_probability(snippet, claim)\n",
    "            if prob > best_entailment:\n",
    "                best_entailment = prob\n",
    "                best_snippet = snippet\n",
    "    \n",
    "    google_is_hallucination = (best_entailment < threshold)\n",
    "    return {\n",
    "        \"claim\": claim,\n",
    "        \"google_snippet\": best_snippet,\n",
    "        \"google_entailment\": best_entailment,\n",
    "        \"google_is_hallucination\": google_is_hallucination\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_claim_with_google_entire_claim(claim, api_key, cx, threshold=0.7, max_results=3):\n",
    "    \"\"\"\n",
    "    Final fallback:\n",
    "    1) Use the entire claim as the Google search query.\n",
    "    2) Retrieve short snippets from the top results.\n",
    "    3) Compute the best entailment score across those snippets.\n",
    "    4) If best >= threshold => not hallucination.\n",
    "    \"\"\"\n",
    "    best_entailment = 0.0\n",
    "    best_snippet = None\n",
    "\n",
    "    # Single query: the entire claim\n",
    "    google_snippets = retrieve_google_snippets(claim, api_key, cx, max_results=max_results)\n",
    "\n",
    "    # Check each returned snippet for entailment\n",
    "    for snippet in google_snippets:\n",
    "        prob = compute_entailment_probability(snippet, claim)\n",
    "        if prob > best_entailment:\n",
    "            best_entailment = prob\n",
    "            best_snippet = snippet\n",
    "\n",
    "    google_is_hallucination = (best_entailment < threshold)\n",
    "    return {\n",
    "        \"claim\": claim,\n",
    "        \"google_snippet\": best_snippet,\n",
    "        \"google_entailment\": best_entailment,\n",
    "        \"google_is_hallucination\": google_is_hallucination\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_check_pipeline(\n",
    "    generated_text, \n",
    "    source_document, \n",
    "    threshold=0.7, \n",
    "    google_api_key=\"YOUR_API_KEY\", \n",
    "    google_cx=\"YOUR_SEARCH_ENGINE_ID\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Split text into claims.\n",
    "    2. For each claim:\n",
    "       - Classify subjectivity with our new model.\n",
    "       - If subjective => skip fact-check.\n",
    "       - Else, check source => if hallucination, check Wikipedia => if STILL hallucination, check Google.\n",
    "    3. Return final results + counts of hallucinations after each stage.\n",
    "    \"\"\"\n",
    "    claims = extract_claims_spacy(generated_text)\n",
    "    final_results = []\n",
    "    \n",
    "    source_hallucinations = 0\n",
    "    wiki_hallucinations = 0\n",
    "    final_hallucinations = 0\n",
    "    \n",
    "    for claim in claims:\n",
    "        # Step A: Subjectivity\n",
    "        sub_label = classify_subjectivity(claim, model, tokenizer, device=device)\n",
    "        is_subjective = (sub_label == 1)\n",
    "        \n",
    "        if is_subjective:\n",
    "            # Skip fact-check\n",
    "            result = {\n",
    "                \"claim\": claim,\n",
    "                \"is_subjective\": True,\n",
    "                \"source_entailment\": None,\n",
    "                \"wikipedia_entailment\": None,\n",
    "                \"google_entailment\": None,\n",
    "                \"is_hallucination\": False\n",
    "            }\n",
    "            final_results.append(result)\n",
    "            continue\n",
    "        \n",
    "        # Step B: Source check\n",
    "        source_result = check_claim_with_source(claim, source_document, threshold=threshold)\n",
    "        if source_result[\"source_is_hallucination\"]:\n",
    "            source_hallucinations += 1\n",
    "            \n",
    "            # Step C: Wikipedia fallback\n",
    "            wiki_result = check_claim_with_wikipedia(claim, threshold=threshold)\n",
    "            if wiki_result[\"wikipedia_is_hallucination\"]:\n",
    "                wiki_hallucinations += 1\n",
    "                \n",
    "                # Step D: Google fallback\n",
    "                google_result = check_claim_with_google(\n",
    "                    claim, \n",
    "                    google_api_key, \n",
    "                    google_cx, \n",
    "                    threshold=threshold\n",
    "                )\n",
    "                \n",
    "                if google_result[\"google_is_hallucination\"]:\n",
    "                    final_hallucinations += 1\n",
    "                    is_hallucination_final = True\n",
    "                else:\n",
    "                    is_hallucination_final = False\n",
    "                \n",
    "                final_results.append({\n",
    "                    \"claim\": claim,\n",
    "                    \"is_subjective\": False,\n",
    "                    \"source_entailment\": source_result[\"source_entailment\"],\n",
    "                    \"wikipedia_entailment\": wiki_result[\"wikipedia_entailment\"],\n",
    "                    \"google_entailment\": google_result[\"google_entailment\"],\n",
    "                    \"is_hallucination\": is_hallucination_final\n",
    "                })\n",
    "            \n",
    "            else:\n",
    "                # Wikipedia saved it from being a hallucination\n",
    "                final_results.append({\n",
    "                    \"claim\": claim,\n",
    "                    \"is_subjective\": False,\n",
    "                    \"source_entailment\": source_result[\"source_entailment\"],\n",
    "                    \"wikipedia_entailment\": wiki_result[\"wikipedia_entailment\"],\n",
    "                    \"google_entailment\": None,\n",
    "                    \"is_hallucination\": False\n",
    "                })\n",
    "        else:\n",
    "            # Source doc already confirms\n",
    "            final_results.append({\n",
    "                \"claim\": claim,\n",
    "                \"is_subjective\": False,\n",
    "                \"source_entailment\": source_result[\"source_entailment\"],\n",
    "                \"wikipedia_entailment\": None,\n",
    "                \"google_entailment\": None,\n",
    "                \"is_hallucination\": False\n",
    "            })\n",
    "    \n",
    "    return final_results, source_hallucinations, wiki_hallucinations, final_hallucinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text=df[\"news\"][10]\n",
    "source_document=df[\"text\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himel/miniconda3/envs/graco/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/himel/miniconda3/envs/graco/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Claims: 32\n",
      "Hallucinations after source check: 12\n",
      "Hallucinations after Wikipedia check: 12\n",
      "Hallucinations after Google check: 12\n",
      "-------------------------------------------------------\n",
      "CLAIM: A Beacon of Hope: Elderly Woman Defies Odds, Beats COVID-19\n",
      "\n",
      "\n",
      "In a poignant tale of resilience and determination, an elderly woman has defied the odds and successfully battled the deadly coronavirus, offering a glimmer of hope in these challenging times.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: The pandemic has had a particularly harsh impact on older populations, with those over 65 accounting for 80% of COVID-19 deaths in the United States alone.\n",
      "Source Entailment: 6.668899732176214e-05\n",
      "Wikipedia Entailment: 0.043790269643068314\n",
      "Google Entailment: 0.001062968629412353\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: However, this 72-year-old woman from Omaha, Nebraska, has proven that age is not a barrier to recovery.\n",
      "Source Entailment: 0.00013677884999196976\n",
      "Wikipedia Entailment: 0.0041793156415224075\n",
      "Google Entailment: 0.000603414315264672\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: Ms. Lila Thompson was admitted to Nebraska Medical Center's Infectious Disease Hospital on April 5 after testing positive for the virus.\n",
      "Source Entailment: 0.11284090578556061\n",
      "Wikipedia Entailment: 0.027338987216353416\n",
      "Google Entailment: 0.010487270541489124\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: With a history of hypertension and diabetes, she was considered high-risk.\n",
      "Source Entailment: 0.0005192177486605942\n",
      "Wikipedia Entailment: 0.0\n",
      "Google Entailment: 0.029505878686904907\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: Yet, with the unwavering support of her family and medical team, she embarked on a journey that would test her strength and spirit.\n",
      "Source Entailment: 0.0011981985298916698\n",
      "Wikipedia Entailment: 0.00561130978167057\n",
      "Google Entailment: 0.004699474200606346\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"I knew it was going to be tough,\" Thompson said in an interview from her hospital bed.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"But I had my faith, my family, and the doctors who were taking good care of me.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: I couldn't give up.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"\n",
      "\n",
      "\n",
      "Thompson's journey mirrors that of millions worldwide who have been affected by the virus.\n",
      "Source Entailment: 0.016302919015288353\n",
      "Wikipedia Entailment: 0.0\n",
      "Google Entailment: 0.05649492144584656\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: The initial symptoms - fever, cough, and fatigue - struck her unawares.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"I thought I was just having a bad cold,\" she recalled.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: But as her condition worsened, it became clear that she had contracted the virus.\n",
      "Source Entailment: 0.002558818319812417\n",
      "Wikipedia Entailment: 0.0009675831533968449\n",
      "Google Entailment: 0.0030718117486685514\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: The virus's effects on older people can be devastating.\n",
      "Source Entailment: 0.0006363863940350711\n",
      "Wikipedia Entailment: 0.004706571809947491\n",
      "Google Entailment: 0.002253135899081826\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: The immune system weakens with age, making older individuals more susceptible to severe illness and complications.\n",
      "Source Entailment: 0.009267664514482021\n",
      "Wikipedia Entailment: 0.0\n",
      "Google Entailment: 0.0257487203925848\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: However, Thompson's story serves as a testament to the human spirit's indomitability.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: Thompson's medical team was instrumental in her recovery.\n",
      "Source Entailment: 0.006716046016663313\n",
      "Wikipedia Entailment: 0.0\n",
      "Google Entailment: 0.011109680868685246\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: Dr. Emily Chang, an infectious disease specialist at Nebraska Medical Center, praised Thompson's resilience.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"Ms. Thompson's case underscores the importance of hope and perseverance,\" Chang said.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"She never gave up, and that made all the difference.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"\n",
      "\n",
      "\n",
      "Thompson's recovery is a reminder of the power of hope in overcoming adversity.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: As the world grapples with the pandemic, stories like Thompson's offer a glimmer of light in the darkness.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: They serve as a beacon, inspiring us to keep fighting and reminding us that there is always hope.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: In these challenging times, it is stories like Thompson's that remind us of the resilience of the human spirit.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: Despite the odds, despite the fear, we can overcome.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: We can recover.\n",
      "Source Entailment: 0.003845405299216509\n",
      "Wikipedia Entailment: 0.26740530133247375\n",
      "Google Entailment: 0.3825328052043915\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: And in doing so, we can inspire others to do the same.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: As Thompson prepares to leave the hospital and return home to her family, she reflects on her experience with gratitude.\n",
      "Source Entailment: 0.001579718547873199\n",
      "Wikipedia Entailment: 0.0\n",
      "Google Entailment: 0.0050829192623496056\n",
      "Final Hallucination? True\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"I'm just thankful,\" she said.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"Thankful for my family, for the doctors and nurses who took care of me, and for the hope that I never lost.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: \"\n",
      "\n",
      "\n",
      "Thompson's story is a testament to the power of hope, resilience, and the indomitable human spirit.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n",
      "CLAIM: It serves as a beacon of inspiration in these challenging times, reminding us that no matter how difficult the journey may seem, recovery is possible.\n",
      "=> Marked as subjective, no fact-check performed.\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Suppose you have your Google credentials\n",
    "    GOOGLE_API_KEY = \"AIzaSyBQNt7wJLePi6oWGGRe07i0oG0VkF_10lc\"\n",
    "    GOOGLE_CX = \"c58bf6758fa324380\"\n",
    "\n",
    "    \n",
    "    results, source_hallu_count, wiki_hallu_count, final_hallu_count = fact_check_pipeline(\n",
    "        generated_text,\n",
    "        source_document,\n",
    "        threshold=0.7,\n",
    "        google_api_key=GOOGLE_API_KEY,\n",
    "        google_cx=GOOGLE_CX\n",
    "    )\n",
    "\n",
    "    print(\"Total Claims:\", len(results))\n",
    "    print(\"Hallucinations after source check:\", source_hallu_count)\n",
    "    print(\"Hallucinations after Wikipedia check:\", wiki_hallu_count)\n",
    "    print(\"Hallucinations after Google check:\", final_hallu_count)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    \n",
    "    for r in results:\n",
    "        print(\"CLAIM:\", r[\"claim\"])\n",
    "        if r[\"is_subjective\"]:\n",
    "            print(\"=> Marked as subjective, no fact-check performed.\")\n",
    "        else:\n",
    "            print(\"Source Entailment:\", r[\"source_entailment\"])\n",
    "            print(\"Wikipedia Entailment:\", r[\"wikipedia_entailment\"])\n",
    "            print(\"Google Entailment:\", r[\"google_entailment\"])\n",
    "            print(\"Final Hallucination?\", r[\"is_hallucination\"])\n",
    "        print(\"-------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on all the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pipeline_outputs = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    generated_text = row[\"news\"]\n",
    "    source_document = row[\"text\"]\n",
    "\n",
    "    # Run the pipeline\n",
    "    # fact_check_pipeline is the function that does:\n",
    "    #  - subjectivity detection\n",
    "    #  - source document check\n",
    "    #  - Wikipedia fallback\n",
    "    #  - Google fallback (final)\n",
    "    results, source_hallu_count, wiki_hallu_count, final_hallu_count = fact_check_pipeline(\n",
    "        generated_text,\n",
    "        source_document,\n",
    "        threshold=0.7,            # or another threshold you choose\n",
    "        google_api_key=GOOGLE_API_KEY,\n",
    "        google_cx=GOOGLE_CX\n",
    "    )\n",
    "    \n",
    "    # Store the outputs in a structured format:\n",
    "    all_pipeline_outputs.append({\n",
    "        \"row_index\": idx,\n",
    "        \"results\": results,                          # full detail of each claim\n",
    "        \"source_hallucinations\": source_hallu_count,  # total # flagged after source check\n",
    "        \"wiki_hallucinations\": wiki_hallu_count,      # total # still flagged after wiki\n",
    "        \"final_hallucinations\": final_hallu_count     # total # still flagged after google\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame if desired\n",
    "df_pipeline_results = pd.DataFrame(all_pipeline_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>results</th>\n",
       "      <th>source_hallucinations</th>\n",
       "      <th>wiki_hallucinations</th>\n",
       "      <th>final_hallucinations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'claim': 'Title: Enthusiasm for Trump's Poli...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'claim': 'Title: Deepening Concerns Over Tru...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'claim': 'Title: Deepening Concerns Over Tru...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'claim': 'Title: Stanford vs. Georgetown: Th...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[{'claim': 'Exploring the Shift in the Diamond...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[{'claim': 'Title: Unpacking the Enigma That I...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[{'claim': 'Headline: Timothée Chalamet's Risi...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[{'claim': 'Title: A Timeless Gaze: The Enduri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[{'claim': 'Title: America's Economic and Poli...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[{'claim': 'Title: U.S. Struggles to Overcome ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[{'claim': 'A Beacon of Hope: Elderly Woman De...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_index                                            results  \\\n",
       "0           0  [{'claim': 'Title: Enthusiasm for Trump's Poli...   \n",
       "1           1  [{'claim': 'Title: Deepening Concerns Over Tru...   \n",
       "2           2  [{'claim': 'Title: Deepening Concerns Over Tru...   \n",
       "3           3  [{'claim': 'Title: Stanford vs. Georgetown: Th...   \n",
       "4           4  [{'claim': 'Exploring the Shift in the Diamond...   \n",
       "5           5  [{'claim': 'Title: Unpacking the Enigma That I...   \n",
       "6           6  [{'claim': 'Headline: Timothée Chalamet's Risi...   \n",
       "7           7  [{'claim': 'Title: A Timeless Gaze: The Enduri...   \n",
       "8           8  [{'claim': 'Title: America's Economic and Poli...   \n",
       "9           9  [{'claim': 'Title: U.S. Struggles to Overcome ...   \n",
       "10         10  [{'claim': 'A Beacon of Hope: Elderly Woman De...   \n",
       "\n",
       "    source_hallucinations  wiki_hallucinations  final_hallucinations  \n",
       "0                       6                    6                     6  \n",
       "1                       5                    5                     5  \n",
       "2                       5                    5                     5  \n",
       "3                      12                   12                    12  \n",
       "4                      16                   16                    16  \n",
       "5                       3                    3                     3  \n",
       "6                       5                    5                     5  \n",
       "7                       0                    0                     0  \n",
       "8                       8                    8                     6  \n",
       "9                       5                    5                     5  \n",
       "10                     12                   12                    12  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pipeline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_pipeline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Claims']=[19,20,20,26,24,22,25,20,26,20,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hallu'] = df.apply(lambda row: (row['final_hallucinations'] / row['Claims'])*100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hallu'] = df['hallu'].apply(lambda x: round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>results</th>\n",
       "      <th>source_hallucinations</th>\n",
       "      <th>wiki_hallucinations</th>\n",
       "      <th>final_hallucinations</th>\n",
       "      <th>Claims</th>\n",
       "      <th>hallu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'claim': 'Title: Enthusiasm for Trump's Poli...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>31.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'claim': 'Title: Deepening Concerns Over Tru...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'claim': 'Title: Deepening Concerns Over Tru...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'claim': 'Title: Stanford vs. Georgetown: Th...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>46.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[{'claim': 'Exploring the Shift in the Diamond...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[{'claim': 'Title: Unpacking the Enigma That I...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>13.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[{'claim': 'Headline: Timothée Chalamet's Risi...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[{'claim': 'Title: A Timeless Gaze: The Enduri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[{'claim': 'Title: America's Economic and Poli...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>23.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[{'claim': 'Title: U.S. Struggles to Overcome ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[{'claim': 'A Beacon of Hope: Elderly Woman De...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>37.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_index                                            results  \\\n",
       "0           0  [{'claim': 'Title: Enthusiasm for Trump's Poli...   \n",
       "1           1  [{'claim': 'Title: Deepening Concerns Over Tru...   \n",
       "2           2  [{'claim': 'Title: Deepening Concerns Over Tru...   \n",
       "3           3  [{'claim': 'Title: Stanford vs. Georgetown: Th...   \n",
       "4           4  [{'claim': 'Exploring the Shift in the Diamond...   \n",
       "5           5  [{'claim': 'Title: Unpacking the Enigma That I...   \n",
       "6           6  [{'claim': 'Headline: Timothée Chalamet's Risi...   \n",
       "7           7  [{'claim': 'Title: A Timeless Gaze: The Enduri...   \n",
       "8           8  [{'claim': 'Title: America's Economic and Poli...   \n",
       "9           9  [{'claim': 'Title: U.S. Struggles to Overcome ...   \n",
       "10         10  [{'claim': 'A Beacon of Hope: Elderly Woman De...   \n",
       "\n",
       "    source_hallucinations  wiki_hallucinations  final_hallucinations  Claims  \\\n",
       "0                       6                    6                     6      19   \n",
       "1                       5                    5                     5      20   \n",
       "2                       5                    5                     5      20   \n",
       "3                      12                   12                    12      26   \n",
       "4                      16                   16                    16      24   \n",
       "5                       3                    3                     3      22   \n",
       "6                       5                    5                     5      25   \n",
       "7                       0                    0                     0      20   \n",
       "8                       8                    8                     6      26   \n",
       "9                       5                    5                     5      20   \n",
       "10                     12                   12                    12      32   \n",
       "\n",
       "    hallu  \n",
       "0   31.58  \n",
       "1   25.00  \n",
       "2   25.00  \n",
       "3   46.15  \n",
       "4   66.67  \n",
       "5   13.64  \n",
       "6   20.00  \n",
       "7    0.00  \n",
       "8   23.08  \n",
       "9   25.00  \n",
       "10  37.50  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"hallucination_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
