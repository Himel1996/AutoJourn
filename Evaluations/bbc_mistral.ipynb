{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ollama\n",
    "import torch\n",
    "import hdbscan\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/himel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/himel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/himel/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"bbc_text_cls.csv\")\n",
    "\n",
    "# Extend stopwords\n",
    "extra_stopwords = {\"said\", \"year\", \"people\", \"new\", \"time\", \"play\", \"told\"}\n",
    "stop_words = set(stopwords.words('english')).union(extra_stopwords)\n",
    "\n",
    "# Initialize NLP tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Function to clean and preprocess text.\"\"\"\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]  # Lemmatization\n",
    "    return \" \".join(words)  # Return string format for BERTopic\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DeepSeek Model\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_embeddings(texts):\n",
    "    \"\"\"Encodes text using Ollama Mistral embeddings.\"\"\"\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Generating embeddings\"):\n",
    "        response = ollama.embeddings(model=\"mistral\", prompt=text)\n",
    "        embeddings.append(response[\"embedding\"])\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 2225/2225 [02:59<00:00, 12.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "embeddings = get_ollama_embeddings(df[\"clean_text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings as a .npy file (Best for reloading in Python)\n",
    "np.save(\"bbc_mistral_embeddings.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN clustering to discover topics\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=30, min_samples=5, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
    "clusters = clusterer.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top words per topic using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words for each cluster\n",
    "num_words = 10\n",
    "topics_words = []\n",
    "for cluster in np.unique(clusters):\n",
    "    if cluster == -1:  # Ignore noise points\n",
    "        continue\n",
    "    cluster_docs = df[df[\"topic\"] == cluster][\"clean_text\"]\n",
    "    cluster_vectorized = vectorizer.transform(cluster_docs)\n",
    "    top_word_indices = np.argsort(cluster_vectorized.toarray().sum(axis=0))[-num_words:]\n",
    "    top_words = [feature_names[i] for i in top_word_indices]\n",
    "    topics_words.append(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Topics: [['tax', 'tory', 'government', 'film', 'party', 'game', 'blair', 'brown', 'election', 'labour'], ['wale', 'try', 'england', 'italy', 'goal', 'penalty', 'ireland', 'half', 'ball', 'minute']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracted Topics:\", topics_words)\n",
    "if len(topics_words) == 0:\n",
    "    print(\"⚠️ No valid topics were found! Check HDBSCAN clustering.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute NPMI Coherence Score\n",
    "id2word = Dictionary(df[\"clean_text\"].apply(str.split))  # Gensim dictionary\n",
    "corpus = [id2word.doc2bow(text.split()) for text in df[\"clean_text\"]]  # Convert text to BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model = CoherenceModel(topics=topics_words, texts=df[\"clean_text\"].apply(str.split), dictionary=id2word, coherence='c_npmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "npmi_score = coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Topic Diversity (TD) Score\n",
    "unique_words = set(word for topic in topics_words for word in topic)  # Unique words across topics\n",
    "total_words = len(topics_words) * num_words  # Total top words\n",
    "td_score = len(unique_words) / total_words  # TD = Unique words / Total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 NPMI Coherence Score: 0.1000\n",
      "🔹 Topic Diversity (TD) Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Display Results\n",
    "print(f\"🔹 NPMI Coherence Score: {npmi_score:.4f}\")\n",
    "print(f\"🔹 Topic Diversity (TD) Score: {td_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    \"NPMI Score\": npmi_score,\n",
    "    \"Topic Diversity\": td_score\n",
    "}\n",
    "pd.DataFrame([results]).to_csv(\"bbc_ollama_mistral_topic_evaluation.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
